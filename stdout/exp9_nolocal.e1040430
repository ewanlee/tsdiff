[2022-11-05 05:18:08,703::train::INFO] Namespace(config='configs/exp9/ts_dv3_newedge_nolocal.yml', device='cuda', fn='_', logdir='./logs', name='exp9_nolocal', pretrain='', project='TS-geom-exp9', resume_iter=None, tag='exp9_nolocal')
[2022-11-05 05:18:08,703::train::INFO] {'model': {'type': 'diffusion', 'network': 'dualenc_ver3_newedge_nolocal', 'hidden_dim': 128, 'num_convs': 6, 'num_convs_local': 4, 'cutoff': 6.0, 'mlp_act': 'ReLU', 'beta_schedule': 'sigmoid', 'beta_start': 1e-07, 'beta_end': 0.002, 'num_diffusion_timesteps': 5000, 'edge_order': 3, 'edge_encoder': 'mlp', 'smooth_conv': True, 'TS': True, 'feat_dim': 22, 'edge_cat_act': 'swish', 'dropout': 0.1}, 'train': {'seed': 2022, 'batch_size': 200, 'val_freq': 1000, 'max_iters': 300000, 'max_grad_norm': 3000.0, 'anneal_power': 2.0, 'optimizer': {'type': 'adam', 'lr': 0.001, 'weight_decay': 0.0, 'beta1': 0.95, 'beta2': 0.999}, 'scheduler': {'type': 'plateau', 'factor': 0.8, 'patience': 10, 'min_lr': 2e-05}}, 'dataset': {'train': 'data/TS/isomerization/random_split/train_data.pkl', 'val': 'data/TS/isomerization/random_split/val_data.pkl', 'test': 'data/TS/isomerization/random_split/test_data.pkl'}}
wandb: Currently logged in as: seonghann. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.4
wandb: Run data is saved locally in /home/ksh/MolDiff/GeoDiff/wandb/run-20221105_051810-1vbdsv05
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exp9_nolocal
wandb: ‚≠êÔ∏è View project at https://wandb.ai/seonghann/TS-geom-exp9
wandb: üöÄ View run at https://wandb.ai/seonghann/TS-geom-exp9/runs/1vbdsv05
[2022-11-05 05:18:15,579::train::INFO] Loading datasets...
[2022-11-05 05:18:19,435::train::INFO] Building model...
[2022-11-05 05:21:12,830::train::INFO] [Train] Iter 01000 | Loss 107.87 | Loss(Global) 107.87 | Loss(Local) 0.00 | Grad 625.93 | LR 0.001000
[2022-11-05 05:21:13,796::train::INFO] [Validate] Iter 01000 | Loss 89.787384 | Loss(Global) 89.787384 | Loss(Local) 0.000000
[2022-11-05 05:24:04,681::train::INFO] [Train] Iter 02000 | Loss 79.19 | Loss(Global) 79.19 | Loss(Local) 0.00 | Grad 629.64 | LR 0.001000
[2022-11-05 05:24:05,648::train::INFO] [Validate] Iter 02000 | Loss 76.667274 | Loss(Global) 76.667274 | Loss(Local) 0.000000
[2022-11-05 05:26:56,349::train::INFO] [Train] Iter 03000 | Loss 73.16 | Loss(Global) 73.16 | Loss(Local) 0.00 | Grad 561.07 | LR 0.001000
[2022-11-05 05:26:57,301::train::INFO] [Validate] Iter 03000 | Loss 70.778751 | Loss(Global) 70.778751 | Loss(Local) 0.000000
[2022-11-05 05:29:46,892::train::INFO] [Train] Iter 04000 | Loss 68.07 | Loss(Global) 68.07 | Loss(Local) 0.00 | Grad 541.39 | LR 0.001000
[2022-11-05 05:29:47,858::train::INFO] [Validate] Iter 04000 | Loss 70.793186 | Loss(Global) 70.793186 | Loss(Local) 0.000000
[2022-11-05 05:32:36,252::train::INFO] [Train] Iter 05000 | Loss 64.80 | Loss(Global) 64.80 | Loss(Local) 0.00 | Grad 514.82 | LR 0.001000
[2022-11-05 05:32:37,118::train::INFO] [Validate] Iter 05000 | Loss 66.045580 | Loss(Global) 66.045580 | Loss(Local) 0.000000
